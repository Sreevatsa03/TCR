\chapter{INTRODUCTION\label{chapter:introduction}}

Adaptive immune system protects vertebrates by detecting and neutralizing foreign invaders (antigens) using T-cell receptors (TCRs), which are placed on the surface of a T-cell \cite{b3}. A T cell is a sub-type of white blood cell that plays a central role in recognizing and neutralizing fragments of antigen with the TCRs. A TCR recognizes an antigen by detecting the small protein fragments that are on the surface of that antigen, and then sends a massage to the nucleus of its T-cell. This successful recognition induces a response for eliminating antigens \cite{b9}. The diversity in the TCR pool increases the chance of detecting a variety of antigens for the adaptive immune system, which is the first step of a successful recovery from diseases. Analysis of TCR pool (repertoire) is crucial for understanding the functionality of healthy immune system, determining the nature of successful and unsuccessful immune responses, and understanding the immune mechanism in presence of different diseases such as type I diabetes, various cancers (blood, breast, colorectal, etc.), rheumatoid arthritis (autoimmune disease), and  multiple sclerosis \cite{b6}. The response of immune system to specific antigen often leaves evidence in the form of repertoire sequence patterns (signatures) that are common across individuals and these signature patterns can be associated with the corresponding antigen. Identification of these signatures help biologists to understand the correlation between the immune receptors and different disease, which provides researchers with the ability to identify immune receptor clones that can be converted into precision vaccines \cite{b1, b7, b8}.

A diverse set of TCRs is required for the adaptive immune system to successfully detect wide variety of antigens. This diversity is achieved by the immune systems of the vertebrates through the DNA recombination process, which is known as the $V(D)J$ recombination \cite{b4}. This process involves rearrangement of variable ($V$), diversity ($D$), and joining ($J$) gene segments in a combinatorial way chosen from members of each gene family \cite{b4,b5}. The form and length of each gene segment varies across different species, and it is more complex in the human than vertebrates. For example, there are 20 different $V$ genes in the mice, while there are 50 different $V$ genes in human \cite{b37}. 
%Combinations of $V$, $D$, and $J$ gene segments of mice generates the TCR repertoire consisting of more than $10^{15}$ sequences. Furthermore, the total number of paths exhausted to generate each combination can exceed $10 ^{18}$. Replicating the recombination process in simulation environment allows immunologists to test different hypothesis on immune system response analysis. However this simulation requires massive scale of data processing.

In the clinical environment, ability to predict the repercussion of immune system to different antigens is a challenging research problem \cite{b37, b17, b18, b19, b20}. For such prediction capability, immunome researchers need a way to model all possible unique TCR sequences (TCR repertoire) that can be generated through the recombination process. This would allow the researchers count the number of times each unique sequence is created through different recombination paths, which forms the baseline for statistical analysis on correlating a specific TCR chain with a specific antigen. However, modeling of all $\alpha\beta$TCR sequences is considerably complex, since the potential TCR repertoire of mouse contains more than $10^{15}$ sequences \cite{b24}. Furthermore, the total number of paths exhausted to generate all the sequences is expected to exceed $10^{18}$. This massive scale of data processing poses as the barrier for immunologists, which has led them to computationally tractable statistical methods \cite{b21}, \cite{b22}  with a trade off  in accuracy \cite{b21}, \cite{b22} . Reducing the timescale of modeling all TCR$\beta$ chains will enable immunologists to test the validity of their assumptions and solve their fundamental questions rapidly such as: What is the real size of the TCR$\beta$ repertoire? Is the procedure of generating TCRs random or biased? What can we say about the frequency distribution of various TCR sequences? The most exhaustive study on TCR synthesis to this date \cite{b2} models more than $10^{14}$ TCR$\beta$ chains by exploiting the data parallel nature of the recombination process and mapping the algorithm entirely on a single graphic processor unit (GPU). This study shows that the execution time can be scaled down to 13 days using an NVIDIA GTX 480 from 52 weeks on a single general purpose processor. This is the first study that models all the potential TCR$\beta$ repertoire of the mouse in which the number of recombination pathways exceeds ${4\times 10^{14}}$.

%In \cite{b2}, authors successfully modeled the mouse $\beta$-TCR repertoire for the first time, in 16 days using an NVIDIA GTX 480 that is expected to take 52 weeks on a general purpose processor based single node system.

For the human system, diversity of the TCR repertoire shows variations from one person to another and increases significantly such that there are more than 50 million TCRs in the immune system of 100 humans. From mouse data set to human data set, the scale of the data increases from $10^{5}$ to $10^{7}$. Furthermore, in humans there are 50 basic $V$ genes, 2 basic $D$ genes, and 13 basic $J$ genes, while in the mouse data set there are 20 $V$ genes, 2 basic $D$ genes, and 12 $J$ genes. From mouse to human, the TCR repertoire increases by three orders of magnitude to $10^{18}$ \cite{b2}. To be able to cope with the scale of the simulations at $10^{18}$ level, our aim in this study is to investigate ways to reduce the execution time and memory footprint of the recombination process so that we can rapidly model systems that are more complex than the mouse. For this, we make the following contributions:

\begin{itemize}
  \item Devising a bit-wise GPU-based implementation of the recombination process, which consists of fine-grained shift, concatenation, comparison, and counting over the binary domain input data set.%Implement bit-wise version of the recombination process as it involves fine-grained shifting, concatenation, comparison, and counting over the input data set.
  \item Extending the bit-wise implementation to a multi-GPU environment and evenly distributing workload across the GPUs being used.  %Expand the implementation to multi-GPU environment and distribute the workload across.
  \item Utilizing the N-level parallelization approach that is used for the GPU-based implementation and implementing the $V(D)J$ recombination algorithm on the field programmable gate array (FPGA).
  \item Devising a VJ-level parallelization method for the FPGA-based implementation to overcome the draw back of N-level parallelization method.   
\end{itemize}



In order to utilize the bit-wise operations, we devise an encoding procedure to convert the input data set from character based domain to binary domain (2-bits per character) and pack a sequence of 4 characters into a single byte. The bit-wise representation of input data set reduces memory access time by a factor of 4, since we can fetch four characters with one memory access. After mapping the input data set to binary domain, we pad the end of input data sequences whose length are not divisible by eight with $0$'s. Correspondingly, we develop a new indexing scheme for addressing the input data stored in the constant memory of GPU to avoid using padded bits of the input data that are not aligned with the byte addressing. Then, we introduce a \emph{task generation} function that generates a unique task for each thread based on its identification number, ensures a unique path for a given sequence by each thread and eliminates the communication between the GPU threads. The two-bit based representation reduces the global and constant memory footprint by factors of 4 and 3.5 compared to the baseline GPU implementation \cite{b2}. Utilizing bit-wise operations and reducing the amount of data transfers reduces the execution time by a factor of 2 using an NVIDIA Tesla P100 GPU. CUDA natively supports 32-bit integer shift and 32-bit bitwise "AND'', "OR'', and "XOR'' operations \cite{b13}, which allows us to implement the bit-wise version of recombination process as it heavily relies on shift, concatenation( using "shift'' and "or'' ) and comparison ("xor'') operations.

For the multi-GPU implementation, we define an \emph{indexing} function, which generates global index for threads in different GPUs based on the index of GPU, and the GPU dimension. Then, we use the proposed \emph{task generation} function to generate unique tasks for each GPU thread. Overall, our aim is to count number of times each \emph{in vivo} sequence can be generated artificially by rearranging input data set ($V$, $D$, $J$ genes and n-nucleotide sequence) to model the TCR repertoire. Therefore, each thread works on its assigned task based on its global index, repeatedly generates a unique sequence, increments the counter for that sequence if it exists in the \emph{in vivo} data set till that thread completes the task of generating all possible sequence based on its given n-nuceleotide. The multi-GPU implementation reduces the execution time from a scale of 18.9 hours to 4.3 hours using 8 GPUs.


As stated above, the recombination process involves intensive amount of fine-grained shift, concatenation, comparison, and counting operations over a data-set generated on the fly. We take advantage of the fine grained parallelism offered by the FPGA whose architecture naturally matches the program architecture of the recombination process. Therefore, we first map the $V(D)J$ recombination algorithm onto the target FPGA using the GPU-based parallelization approach, which is known as N-level method. In order to address this, we first define the degree of parallelism for the final hardware architecture. We also show the relation between degree of parallelism with critical path delay and resource utilization. We form the memory hierarchy based on the selected degree of parallelism and design required components to map the recombination process. In addition, we utilize the length and $VJ$ pair index of the generated sequence to pare down the comparison search space. Experimental results for the FPGA implementation show that the N-level parallelization approach causes communication overhead among FPGA components and result in poor performance. Therefore, we devise a VJ-level parallelization approach to form a unique memory hierarchy to match the data access patterns for various stages of the algorithm, which results in the elimination of communication overhead among components. We show that the VJ-level parallelization approach accelerate the recombination process by a factor of 2.34 in comparison with the N-level method for the FPGA-based implementation.

The  rest  of  this  dissertation  is  organized  as  follows:   Chapter \ref{sec:DNA}  provides  an overview  of  the DNA recombination algorithm from biological and algorithmic perspectives. Chapter \ref{Chapter:GPU} describes a bit-wise implementation of the recombination process on a single and multi-GPU. A detailed explanation of the FPGA-based implementation of the $V(D)J$ recombination process is provided in Chapter \ref{chapter:FPGA}. Conclusions and future work are discussed in Chapter \ref{sec:Conclusion}.

%\section{Overview\label{sec:overview}}
%
%Some text.